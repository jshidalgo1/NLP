{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tuning mT5-base on Cebuano GSM8K with LoRA\n",
                "\n",
                "This notebook fine-tunes the `google/mt5-base` model on the translated Cebuano GSM8K dataset (`gsm8k_cebuano.jsonl`).\n",
                "It uses **LoRA (Low-Rank Adaptation)** to efficiently train the model on a consumer GPU (like in Google Colab)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets peft accelerate bitsandbytes sentencepiece"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "import os\n",
                "\n",
                "# Check for GPU\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_FILE = \"gsm8k_cebuano.jsonl\"\n",
                "\n",
                "if not os.path.exists(DATA_FILE):\n",
                "    print(f\"WARNING: {DATA_FILE} not found. Please upload your translated file.\")\n",
                "else:\n",
                "    dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
                "    print(f\"Loaded {len(dataset)} examples.\")\n",
                "    \n",
                "    # Optional: Filter by similarity score if available\n",
                "    if \"similarity_score\" in dataset.column_names:\n",
                "        original_len = len(dataset)\n",
                "        dataset = dataset.filter(lambda x: x[\"similarity_score\"] >= 0.75)\n",
                "        print(f\"Filtered to {len(dataset)} examples (score >= 0.75).\")\n",
                "    \n",
                "    # Split into train/val\n",
                "    dataset = dataset.train_test_split(test_size=0.1)\n",
                "    print(dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_ID = \"google/mt5-base\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "\n",
                "def preprocess_function(examples):\n",
                "    # Format: \"Question: <question>\"\n",
                "    inputs = [f\"Question: {q}\" for q in examples[\"cebuano_question\"]]\n",
                "    targets = examples[\"cebuano_answer\"]\n",
                "    \n",
                "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
                "    labels = tokenizer(targets, max_length=512, truncation=True)\n",
                "    \n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Setup (LoRA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model in 8-bit to save memory\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    load_in_8bit=True,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "# Prepare for k-bit training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# LoRA Configuration\n",
                "lora_config = LoraConfig(\n",
                "    r=8,\n",
                "    lora_alpha=16,\n",
                "    target_modules=[\"q\", \"v\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.SEQ_2_SEQ_LM\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"mt5-gsm8k-cebuano\",\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=1e-3,\n",
                "    num_train_epochs=3,\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    fp16=True, # Use mixed precision\n",
                ")\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset[\"train\"],\n",
                "    eval_dataset=tokenized_dataset[\"test\"],\n",
                "    data_collator=data_collator,\n",
                ")\n",
                "\n",
                "# Train\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save and Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save_pretrained(\"mt5-gsm8k-cebuano-final\")\n",
                "\n",
                "def solve_problem(question):\n",
                "    inputs = tokenizer(f\"Question: {question}\", return_tensors=\"pt\").to(device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_length=256)\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test\n",
                "sample_q = dataset[\"test\"][0][\"cebuano_question\"]\n",
                "print(f\"Question: {sample_q}\")\n",
                "print(f\"Answer: {solve_problem(sample_q)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}