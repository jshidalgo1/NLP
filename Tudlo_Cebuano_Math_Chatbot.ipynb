{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "14d8e8be",
            "metadata": {},
            "source": [
                "# Tudlo: Cebuano Math Chatbot (Upgraded)\n",
                "\n",
                "This notebook implements the **Tudlo** system for a Cebuano Math Chatbot, based on the research paper *\"Tudlo: Parameter-Efficient Fine-tuning of a Multilingual Transformer for Cebuano Mathematics Education\"*.\n",
                "\n",
                "## Key Upgrades Implemented:\n",
                "1.  **Model**: Upgraded from `mT5-small` to **`mT5-base`** for enhanced reasoning capabilities.\n",
                "2.  **Data Quality**: Implemented an **Automated Back-Translation Loop** using **GoogleTrans** (English -> Cebuano -> English) and **Semantic Similarity Filtering** to ensure high-quality training data.\n",
                "3.  **Evaluation**: Added **Numerical Exact Match** metric alongside standard text generation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dca7898d",
            "metadata": {},
            "source": [
                "## 1. Dependencies\n",
                "Installing necessary libraries for QLoRA, translation, and training.\n",
                "**Note**: We use `googletrans==4.0.0-rc1` for API stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d183548f",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers peft bitsandbytes accelerate datasets sentencepiece sacremoses sentence-transformers googletrans==4.0.0-rc1"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9aa34320",
            "metadata": {},
            "source": [
                "## 2. Data Pipeline (The Upgrade)\n",
                "We load the GSM8K dataset and use **GoogleTrans** for translation. We filter the results using **Semantic Similarity** to retain only the best pairs. \n",
                "**Note**: Since we are using an external API, we process sequentially with delays to avoid rate limiting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49a21c2e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset, Dataset\n",
                "from googletrans import Translator\n",
                "from sentence_transformers import SentenceTransformer, util\n",
                "import torch\n",
                "from tqdm.auto import tqdm\n",
                "import time\n",
                "import random\n",
                "\n",
                "# 1. Load GSM8K (Scaled up to 500 examples)\n",
                "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train[:500]\")\n",
                "\n",
                "# 2. Initialize Google Translator\n",
                "translator = Translator()\n",
                "\n",
                "# 3. Load Semantic Similarity Model\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Loading Semantic Similarity Model...\")\n",
                "sim_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)\n",
                "\n",
                "def translate_google(text, src, dest, retries=3):\n",
                "    for i in range(retries):\n",
                "        try:\n",
                "            # Add delay to avoid rate limiting\n",
                "            time.sleep(0.5 + random.random()) \n",
                "            result = translator.translate(text, src=src, dest=dest)\n",
                "            return result.text\n",
                "        except Exception as e:\n",
                "            if i == retries - 1:\n",
                "                print(f\"Failed to translate: {e}\")\n",
                "                return None\n",
                "            time.sleep(2) # Wait longer before retry\n",
                "    return None\n",
                "\n",
                "def get_semantic_similarity(text1, text2):\n",
                "    emb1 = sim_model.encode(text1, convert_to_tensor=True)\n",
                "    emb2 = sim_model.encode(text2, convert_to_tensor=True)\n",
                "    return util.pytorch_cos_sim(emb1, emb2).item()\n",
                "\n",
                "# 4. Back-Translation Loop (Sequential)\n",
                "cebuano_data = []\n",
                "similarity_threshold = 0.75\n",
                "\n",
                "print(\"Starting Back-Translation Loop (this may take a while due to API delays)...\")\n",
                "\n",
                "for example in tqdm(gsm8k):\n",
                "    question_en = example['question']\n",
                "    answer_en = example['answer']\n",
                "    \n",
                "    # Translate Question: En -> Ceb -> En\n",
                "    q_ceb = translate_google(question_en, src='en', dest='ceb')\n",
                "    if not q_ceb: continue\n",
                "    \n",
                "    q_back = translate_google(q_ceb, src='ceb', dest='en')\n",
                "    if not q_back: continue\n",
                "    \n",
                "    # Translate Answer: En -> Ceb -> En\n",
                "    a_ceb = translate_google(answer_en, src='en', dest='ceb')\n",
                "    if not a_ceb: continue\n",
                "    \n",
                "    a_back = translate_google(a_ceb, src='ceb', dest='en')\n",
                "    if not a_back: continue\n",
                "    \n",
                "    # Calculate Similarity\n",
                "    score_q = get_semantic_similarity(question_en, q_back)\n",
                "    score_a = get_semantic_similarity(answer_en, a_back)\n",
                "    \n",
                "    if score_q > similarity_threshold and score_a > similarity_threshold:\n",
                "        formatted_prompt = f\"Ipasabot ang solusyon sa matematika sa Cebuano: {q_ceb}\"\n",
                "        cebuano_data.append({\n",
                "            \"input_text\": formatted_prompt,\n",
                "            \"target_text\": a_ceb,\n",
                "            \"original_q_score\": score_q\n",
                "        })\n",
                "\n",
                "print(f\"Retained {len(cebuano_data)}/{len(gsm8k)} high-quality pairs.\")\n",
                "\n",
                "# Create HF Dataset\n",
                "dataset = Dataset.from_list(cebuano_data)\n",
                "dataset = dataset.train_test_split(test_size=0.1)\n",
                "\n",
                "# Data Inspection\n",
                "print(\"\\n--- Data Inspection ---\")\n",
                "if len(dataset['train']) > 0:\n",
                "    for i in range(min(3, len(dataset['train']))):\n",
                "        ex = dataset['train'][i]\n",
                "        print(f\"Example {i+1}:\")\n",
                "        print(f\"Input: {ex['input_text']}\")\n",
                "        print(f\"Target: {ex['target_text']}\")\n",
                "        print(\"-\"*20)\n",
                "else:\n",
                "    print(\"No data retained. Check API connectivity or lower threshold.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1325b7b6",
            "metadata": {},
            "source": [
                "## 3. QLoRA Configuration\n",
                "Configuring `mT5-base` with 4-bit quantization and LoRA adapters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed48b67f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "\n",
                "model_id = \"google/mt5-base\"\n",
                "\n",
                "# 4-bit Quantization Config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "# Load Model\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# LoRA Config (from paper: r=8, alpha=16, dropout=0.05)\n",
                "lora_config = LoraConfig(\n",
                "    r=8,\n",
                "    lora_alpha=16,\n",
                "    target_modules=[\"q\", \"v\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.SEQ_2_SEQ_LM\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5deed156",
            "metadata": {},
            "source": [
                "## 4. Training\n",
                "Setting up the Seq2SeqTrainer with hyperparameters from the paper."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a16a3b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
                "\n",
                "# Tokenize Dataset\n",
                "def preprocess_function(examples):\n",
                "    inputs = [ex for ex in examples[\"input_text\"]]\n",
                "    targets = [ex for ex in examples[\"target_text\"]]\n",
                "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
                "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
                "\n",
                "# Training Arguments\n",
                "# Paper: Batch 4, Grad Accum 4, LR 2e-4, 10 Epochs\n",
                "# Tuned: LR 1e-4 to prevent overfitting on small data\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"./tudlo_checkpoints\",\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=1e-4,\n",
                "    num_train_epochs=10,\n",
                "    logging_steps=10,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    fp16=True,\n",
                "    save_strategy=\"epoch\",\n",
                "    eval_strategy=\"epoch\",\n",
                "    predict_with_generate=True\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_datasets[\"train\"],\n",
                "    eval_dataset=tokenized_datasets[\"test\"],\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b18ca28e",
            "metadata": {},
            "source": [
                "## 5. Inference & Evaluation\n",
                "Generating answers and performing Numerical Exact Match evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9aa94ffa",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "def extract_number(text):\n",
                "    # Simple regex to find the last number in the text, handling decimals\n",
                "    # This is a heuristic for GSM8K where the answer is usually at the end\n",
                "    matches = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n",
                "    if matches:\n",
                "        return float(matches[-1])\n",
                "    return None\n",
                "\n",
                "def generate_answer(question_text):\n",
                "    inputs = tokenizer(question_text, return_tensors=\"pt\").to(device)\n",
                "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"Running Evaluation on Test Set...\")\n",
                "correct = 0\n",
                "total = 0\n",
                "\n",
                "if len(tokenized_datasets[\"test\"]) > 0:\n",
                "    for example in tokenized_datasets[\"test\"]:\n",
                "        # Note: We need the raw text for generation, not the tokenized version\n",
                "        # So we'll grab from the original dataset slice corresponding to the test set\n",
                "        # For simplicity here, we'll just demonstrate with a manual call or iterate the raw dataset if indices matched.\n",
                "        # Let's just pick a few examples from the raw test split:\n",
                "        pass\n",
                "\n",
                "    # Better loop for eval:\n",
                "    test_data = dataset[\"test\"]\n",
                "    for i in range(len(test_data)):\n",
                "        input_text = test_data[i][\"input_text\"]\n",
                "        target_text = test_data[i][\"target_text\"]\n",
                "        \n",
                "        generated_text = generate_answer(input_text)\n",
                "        \n",
                "        pred_num = extract_number(generated_text)\n",
                "        ref_num = extract_number(target_text)\n",
                "        \n",
                "        is_match = (pred_num is not None) and (ref_num is not None) and (abs(pred_num - ref_num) < 1e-6)\n",
                "        if is_match:\n",
                "            correct += 1\n",
                "        total += 1\n",
                "        \n",
                "        print(f\"Q: {input_text}\")\n",
                "        print(f\"Gen: {generated_text}\")\n",
                "        print(f\"Ref: {target_text}\")\n",
                "        print(f\"Match: {is_match} (Pred: {pred_num}, Ref: {ref_num})\")\n",
                "        print(\"-\"*20)\n",
                "\n",
                "    print(f\"Final Accuracy: {correct/total:.2%}\")\n",
                "else:\n",
                "    print(\"Test set is empty. Cannot evaluate.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
