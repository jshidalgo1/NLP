{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GSM8K to Cebuano Translation with QA\n",
                "\n",
                "This notebook translates the GSM8K dataset to Cebuano using the NLLB-200 model. It includes a Quality Assurance (QA) step using back-translation and semantic similarity."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch transformers datasets sentencepiece accelerate protobuf sentence-transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import gc\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "from sentence_transformers import SentenceTransformer, util\n",
                "from tqdm.notebook import tqdm\n",
                "import os\n",
                "\n",
                "def cleanup():\n",
                "    \"\"\"Aggressively clear memory.\"\"\"\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_text(text, model, tokenizer, device, src_lang=\"eng_Latn\", tgt_lang=\"ceb_Latn\"):\n",
                "    \"\"\"Translates a single string.\"\"\"\n",
                "    if not text:\n",
                "        return \"\"\n",
                "    \n",
                "    # Tokenize\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
                "    \n",
                "    # Translate\n",
                "    with torch.no_grad():\n",
                "        generated_tokens = model.generate(\n",
                "            **inputs,\n",
                "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
                "            max_length=512\n",
                "        )\n",
                "    \n",
                "    # Decode\n",
                "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
                "    \n",
                "    # Cleanup tensors immediately\n",
                "    del inputs\n",
                "    del generated_tokens\n",
                "    cleanup()\n",
                "    \n",
                "    return result\n",
                "\n",
                "def get_similarity(text1, text2, sim_model):\n",
                "    \"\"\"Computes cosine similarity between two texts.\"\"\"\n",
                "    if not text1 or not text2:\n",
                "        return 0.0\n",
                "    embeddings = sim_model.encode([text1, text2], convert_to_tensor=True)\n",
                "    score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
                "    del embeddings\n",
                "    cleanup()\n",
                "    return score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Device setup\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load Translation Model & Tokenizer\n",
                "model_name = \"facebook/nllb-200-distilled-600M\"\n",
                "print(f\"Loading translation model: {model_name}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"eng_Latn\")\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "if device == \"cuda\":\n",
                "    model = model.half() # Use fp16 for GPU\n",
                "model = model.to(device)\n",
                "\n",
                "# Load Similarity Model\n",
                "print(\"Loading similarity model: all-MiniLM-L6-v2...\")\n",
                "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "sim_model = sim_model.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "START_INDEX = 100  # Start processing from this index\n",
                "END_INDEX = 200    # Stop processing at this index (None for end of dataset)\n",
                "OUTPUT_FILE = \"gsm8k_cebuano.jsonl\"\n",
                "THRESHOLD = 0.8  # Minimum similarity score to save\n",
                "\n",
                "# Load Dataset (Streaming)\n",
                "print(\"Loading GSM8K dataset (streaming)...\")\n",
                "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\", streaming=True)\n",
                "\n",
                "# Check if file exists to resume (only if starting from 0, otherwise trust user)\n",
                "if START_INDEX == 0 and os.path.exists(OUTPUT_FILE):\n",
                "    with open(OUTPUT_FILE, \"r\") as f:\n",
                "        lines = sum(1 for _ in f)\n",
                "        if lines > 0:\n",
                "            print(f\"Output file exists with {lines} lines. Resuming from index {lines}...\")\n",
                "            START_INDEX = lines\n",
                "\n",
                "print(f\"Processing range: {START_INDEX} to {END_INDEX if END_INDEX else 'End'}\")\n",
                "print(f\"Translating and saving to {OUTPUT_FILE}...\")\n",
                "\n",
                "with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
                "    for i, example in tqdm(enumerate(dataset)):\n",
                "        if i < START_INDEX:\n",
                "            continue\n",
                "        \n",
                "        if END_INDEX is not None and i >= END_INDEX:\n",
                "            print(f\"Reached end index {END_INDEX}. Stopping.\")\n",
                "            break\n",
                "            \n",
                "        original_question = example[\"question\"]\n",
                "        original_answer = example[\"answer\"]\n",
                "        \n",
                "        # 1. Translate to Cebuano\n",
                "        ceb_question = translate_text(original_question, model, tokenizer, device, src_lang=\"eng_Latn\", tgt_lang=\"ceb_Latn\")\n",
                "        ceb_answer = translate_text(original_answer, model, tokenizer, device, src_lang=\"eng_Latn\", tgt_lang=\"ceb_Latn\")\n",
                "        \n",
                "        # 2. Back-translate to English\n",
                "        back_question = translate_text(ceb_question, model, tokenizer, device, src_lang=\"ceb_Latn\", tgt_lang=\"eng_Latn\")\n",
                "        \n",
                "        # 3. Compute Similarity\n",
                "        similarity = get_similarity(original_question, back_question, sim_model)\n",
                "        \n",
                "        # 4. Filter\n",
                "        if similarity < THRESHOLD:\n",
                "            print(f\"Skipping index {i}: Similarity {similarity:.4f} < {THRESHOLD}\")\n",
                "            continue\n",
                "\n",
                "        # Write to file\n",
                "        output_data = {\n",
                "            \"original_question\": original_question,\n",
                "            \"original_answer\": original_answer,\n",
                "            \"cebuano_question\": ceb_question,\n",
                "            \"cebuano_answer\": ceb_answer,\n",
                "            \"back_translation\": back_question,\n",
                "            \"similarity_score\": similarity\n",
                "        }\n",
                "        f.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
                "        f.flush()\n",
                "        \n",
                "        cleanup()\n",
                "\n",
                "print(\"Translation complete.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}